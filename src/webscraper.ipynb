{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-05T21:20:10.780046Z",
     "start_time": "2025-09-05T21:17:55.839802Z"
    }
   },
   "source": [
    "import re\n",
    "import time\n",
    "import csv\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "BASE = \"https://www.aselsan.com\"\n",
    "LIST_URL = \"https://www.aselsan.com/tr/haberler\"\n",
    "\n",
    "# Tek bir oturum kullan (daha hızlı ve nazik) + retry/backoff\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; aselsan-scraper/1.0; +https://example.com)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Referer\": LIST_URL,\n",
    "})\n",
    "retries = Retry(\n",
    "    total=5,\n",
    "    connect=5,\n",
    "    read=5,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "\n",
    "def _get_soup(url):\n",
    "    r = session.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def list_page_urls(page_no=1):\n",
    "    \"\"\"\n",
    "    Haber liste sayfasındaki detay linklerini döndürür.\n",
    "    Sayfa yoksa boş liste döner.\n",
    "    Bazı sitelerde farklı sayfalama parametreleri olabildiği için\n",
    "    birkaç varyasyonu deniyoruz.\n",
    "    \"\"\"\n",
    "    candidate_urls = []\n",
    "    if page_no == 1:\n",
    "        candidate_urls.append(LIST_URL)\n",
    "    else:\n",
    "        candidate_urls.extend([\n",
    "            f\"{LIST_URL}?pageNo={page_no}\",\n",
    "            f\"{LIST_URL}?page={page_no}\",\n",
    "            f\"{LIST_URL}?p={page_no}\",\n",
    "        ])\n",
    "\n",
    "    links = []\n",
    "    for url in candidate_urls:\n",
    "        try:\n",
    "            soup = _get_soup(url)\n",
    "        except requests.HTTPError:\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Liste içinde detay linkleri\n",
    "        for a in soup.select('a[href*=\"/haberler/detay/\"]'):\n",
    "            href = a.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            full = urljoin(BASE, href)\n",
    "            links.append(full)\n",
    "\n",
    "        # Eğer bağlantı bulduysak diğer varyasyonları denemeye gerek yok\n",
    "        if links:\n",
    "            break\n",
    "\n",
    "    return sorted(set(links))\n",
    "\n",
    "\n",
    "def _text_or_none(tag):\n",
    "    return tag.get_text(strip=True) if tag else None\n",
    "\n",
    "\n",
    "def _pick_date_from_meta(soup):\n",
    "    # Fallback: meta[property=\"article:published_time\"] veya benzeri\n",
    "    meta = soup.find(\"meta\", attrs={\"property\": re.compile(r\"(article:published_time|og:updated_time)\")})\n",
    "    if meta and meta.get(\"content\"):\n",
    "        return meta[\"content\"]\n",
    "    # Sayfada tarih formatı: 01.09.2025 / 1 Eylül 2025 gibi\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    m = re.search(r\"\\b(\\d{1,2}\\.\\d{1,2}\\.\\d{4})\\b\", text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    # Türkçe ay isimleri ile basit arama\n",
    "    months = \"Ocak|Şubat|Mart|Nisan|Mayıs|Haziran|Temmuz|Ağustos|Eylül|Ekim|Kasım|Aralık\"\n",
    "    m2 = re.search(rf\"\\b(\\d{{1,2}}\\s+(?:{months})\\s+\\d{{4}})\\b\", text)\n",
    "    return m2.group(1) if m2 else None\n",
    "\n",
    "\n",
    "def _pick_date_box(soup):\n",
    "    \"\"\"Takvim ikonlu tarih kutusunu bulur (varsa).\"\"\"\n",
    "    for box in soup.select(\".box.box-date, .news-date, .date, .meta, .info\"):\n",
    "        icon = box.find(\"i\", class_=re.compile(r\"fa-.*calendar\"))\n",
    "        if icon:\n",
    "            # Kutudaki tarih benzeri ifadeyi izole etmeye çalış\n",
    "            txt = box.get_text(\" \", strip=True)\n",
    "            # Önce dd.mm.yyyy\n",
    "            m = re.search(r\"\\b\\d{1,2}\\.\\d{1,2}\\.\\d{4}\\b\", txt)\n",
    "            if m:\n",
    "                return m.group(0)\n",
    "            # Sonra ay ismi ile\n",
    "            months = \"Ocak|Şubat|Mart|Nisan|Mayıs|Haziran|Temmuz|Ağustos|Eylül|Ekim|Kasım|Aralık\"\n",
    "            m2 = re.search(rf\"\\b\\d{{1,2}}\\s+(?:{months})\\s+\\d{{4}}\\b\", txt)\n",
    "            if m2:\n",
    "                return m2.group(0)\n",
    "            return txt  # hiçbiri tutmazsa kutunun tamamı\n",
    "    return None\n",
    "\n",
    "\n",
    "def _pick_readtime_box(soup):\n",
    "    \"\"\"Saat ikonlu okuma süresi kutusunu bulur (varsa).\"\"\"\n",
    "    for box in soup.select(\".box.box-date, .news-date, .date, .meta, .info\"):\n",
    "        icon = box.find(\"i\", class_=re.compile(r\"fa-.*clock\"))\n",
    "        if icon:\n",
    "            txt = box.get_text(\" \", strip=True)\n",
    "            # 'dk', 'dakika' gibi ifadeleri yakala\n",
    "            m = re.search(r\"(\\d+\\s*(?:dk|dakika))\", txt, flags=re.IGNORECASE)\n",
    "            return m.group(1) if m else txt\n",
    "    # Fallback: tüm sayfada bir arama\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    m = re.search(r\"(\\d+\\s*(?:dk|dakika))\", text, flags=re.IGNORECASE)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def _pick_title(soup):\n",
    "    title_tag = soup.select_one(\"h1.hero-title, h1.page-title, h1.title, h1\")\n",
    "    if title_tag:\n",
    "        return _text_or_none(title_tag)\n",
    "    # meta og:title\n",
    "    meta = soup.find(\"meta\", attrs={\"property\": \"og:title\"})\n",
    "    if meta and meta.get(\"content\"):\n",
    "        return meta[\"content\"].strip()\n",
    "    # h2 fallback\n",
    "    h2 = soup.select_one(\".box.box-content h2, h2\")\n",
    "    return _text_or_none(h2)\n",
    "\n",
    "\n",
    "def _pick_hero_image(soup):\n",
    "    # 1) belirgin hero id/class\n",
    "    hero_img = soup.select_one(\"#news-hero picture img, #news-hero img, .news-hero img, .hero img\")\n",
    "    for img in [hero_img] if hero_img else []:\n",
    "        src = img.get(\"src\") or img.get(\"data-src\") or img.get(\"data-original\")\n",
    "        if src:\n",
    "            return urljoin(BASE, src)\n",
    "\n",
    "    # 2) og:image\n",
    "    meta = soup.find(\"meta\", attrs={\"property\": \"og:image\"})\n",
    "    if meta and meta.get(\"content\"):\n",
    "        return urljoin(BASE, meta[\"content\"])\n",
    "\n",
    "    # 3) içerikte ilk anlamlı görsel\n",
    "    any_img = soup.select_one(\".box.box-content img, article img, .content img, img\")\n",
    "    if any_img:\n",
    "        src = any_img.get(\"src\") or any_img.get(\"data-src\") or any_img.get(\"data-original\")\n",
    "        if src:\n",
    "            return urljoin(BASE, src)\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_detail(url):\n",
    "    \"\"\"\n",
    "    Haber detay sayfasını parse eder.\n",
    "    Dönen dict: id, slug, url, title, date, read_time, hero_image, content_text, content_images\n",
    "    \"\"\"\n",
    "    soup = _get_soup(url)\n",
    "\n",
    "    # id ve slug (URL'den)\n",
    "    m = re.search(r\"/haberler/detay/(\\d+)/([^/?#]+)\", url)\n",
    "    news_id = int(m.group(1)) if m else None\n",
    "    slug = m.group(2) if m else None\n",
    "\n",
    "    # başlık\n",
    "    title = _pick_title(soup)\n",
    "\n",
    "    # tarih & okuma süresi\n",
    "    date_text = _pick_date_box(soup) or _pick_date_from_meta(soup)\n",
    "    read_time = _pick_readtime_box(soup)\n",
    "\n",
    "    # kapak görseli\n",
    "    hero_src = _pick_hero_image(soup)\n",
    "\n",
    "    # içerik gövdesi: birkaç olası kapsayıcı\n",
    "    content_box = (\n",
    "        soup.select_one(\".box.box-content\")\n",
    "        or soup.select_one(\"article\")\n",
    "        or soup.select_one(\".content\")\n",
    "        or soup.select_one(\".news-detail\")\n",
    "    )\n",
    "\n",
    "    paragraphs = []\n",
    "    content_images = []\n",
    "    if content_box:\n",
    "        for p in content_box.select(\"p\"):\n",
    "            txt = p.get_text(\" \", strip=True)\n",
    "            if txt:\n",
    "                paragraphs.append(txt)\n",
    "\n",
    "        for img in content_box.select(\"img\"):\n",
    "            src = img.get(\"src\") or img.get(\"data-src\") or img.get(\"data-original\")\n",
    "            if src:\n",
    "                content_images.append(urljoin(BASE, src))\n",
    "\n",
    "    content_text = \"\\n\\n\".join(paragraphs) if paragraphs else None\n",
    "\n",
    "    return {\n",
    "        \"id\": news_id,\n",
    "        \"slug\": slug,\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"date\": date_text,\n",
    "        \"read_time\": read_time,\n",
    "        \"hero_image\": hero_src,\n",
    "        \"content_text\": content_text,\n",
    "        \"content_images\": content_images,\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_all(max_pages=50, delay=0.8):\n",
    "    \"\"\"\n",
    "    Tüm haberleri sayfa sayfa gezip parse eder.\n",
    "    max_pages: en fazla kaç liste sayfası denensin\n",
    "    delay: her istek arasında bekleme (saniye)\n",
    "    \"\"\"\n",
    "    seen_urls = set()\n",
    "    records = []\n",
    "\n",
    "    for p in range(1, max_pages + 1):\n",
    "        try:\n",
    "            links = list_page_urls(p)\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"[WARN] Liste sayfası {p} HTTP hatası: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Liste sayfası {p} beklenmeyen hata: {e}\")\n",
    "            break\n",
    "\n",
    "        if not links:\n",
    "            # daha fazla sayfa görünmüyor\n",
    "            break\n",
    "\n",
    "        new_in_page = 0\n",
    "        for u in links:\n",
    "            if u in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(u)\n",
    "\n",
    "            try:\n",
    "                rec = parse_detail(u)\n",
    "                records.append(rec)\n",
    "                new_in_page += 1\n",
    "            except requests.HTTPError as e:\n",
    "                print(f\"[WARN] Detay HTTP hatası: {u} -> {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Detay beklenmeyen hata: {u} -> {e}\")\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "        # Güvenlik: bir sayfada hiç yeni link yoksa erken çık\n",
    "        if new_in_page == 0 and p > 1:\n",
    "            break\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def _sanitize_for_csv(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    # CSV’de düzenli görünmesi için olası \\r\\n ve dikey çizgi temizlikleri\n",
    "    return s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl_all(max_pages=60, delay=0.8)\n",
    "\n",
    "    # id'ye göre benzersizleştir (son görüleni al)\n",
    "    by_id = {}\n",
    "    for item in data:\n",
    "        if item and item.get(\"id\") is not None:\n",
    "            by_id[item[\"id\"]] = item\n",
    "    deduped = sorted(by_id.values(), key=lambda x: x[\"id\"])\n",
    "\n",
    "    # CSV yaz\n",
    "    with open(\"aselsan_haberler.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\n",
    "                \"id\",\n",
    "                \"slug\",\n",
    "                \"url\",\n",
    "                \"title\",\n",
    "                \"date\",\n",
    "                \"read_time\",\n",
    "                \"hero_image\",\n",
    "                \"content_text\",\n",
    "                \"content_images\",\n",
    "            ],\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        for row in deduped:\n",
    "            row_out = row.copy()\n",
    "            # listeleri pipe ile birleştir\n",
    "            imgs = row_out.get(\"content_images\") or []\n",
    "            row_out[\"content_images\"] = \"|\".join(imgs)\n",
    "            # metinleri temizle\n",
    "            row_out[\"title\"] = _sanitize_for_csv(row_out.get(\"title\"))\n",
    "            row_out[\"date\"] = _sanitize_for_csv(row_out.get(\"date\"))\n",
    "            row_out[\"read_time\"] = _sanitize_for_csv(row_out.get(\"read_time\"))\n",
    "            row_out[\"hero_image\"] = _sanitize_for_csv(row_out.get(\"hero_image\"))\n",
    "            row_out[\"content_text\"] = _sanitize_for_csv(row_out.get(\"content_text\"))\n",
    "            writer.writerow(row_out)\n",
    "\n",
    "    print(f\"{len(deduped)} haber kaydedildi -> aselsan_haberler.csv\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gundo\\anaconda3\\envs\\py310\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 haber kaydedildi -> aselsan_haberler.csv\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
